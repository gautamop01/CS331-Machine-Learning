{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AyGaRoKa LAB ASSIGNMENT 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classes containing init, forward and backward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASE FUNCTIONS\n",
    "\n",
    "class _multiplication_layer:\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X # X (numpy.ndarray): Input matrix.\n",
    "        self.W = W # W (numpy.ndarray): Weight matrix.\n",
    "\n",
    "    def forward(self):\n",
    "        self.Z = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T\n",
    "        self.dZ_daZ_prev = self.W\n",
    "\n",
    "class _bias_addition_layer:\n",
    "    def __init__(self, Z, b):\n",
    "        self.B = b # b (numpy.ndarray): Bias matrix.\n",
    "        self.Z = Z #  Z (numpy.ndarray): Input matrix.\n",
    "    \n",
    "    def forward(self):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self):\n",
    "        self.dZ_dB = np.identity(self.B.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOSSES\n",
    "\n",
    "class _mean_squared_error_loss:\n",
    "    def __init__(self, Y, Y_hat):\n",
    "        self.Y = Y #  Y (numpy.ndarray): True labels.\n",
    "        self.aZ = Y_hat # Y_hat (numpy.ndarray): Predicted labels.\n",
    "\n",
    "    def forward(self):\n",
    "        self.L = np.mean((self.aZ - self.Y)**2)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T\n",
    "\n",
    "\n",
    "class _cross_entropy_loss:\n",
    "    def __init__(self, Y, Y_pred): # Constructor for the cross-entropy loss.\n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-20\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward pass of the cross-entropy loss.\n",
    "        Computes the cross-entropy loss and stores the result in L.\n",
    "        \"\"\"\n",
    "        self.L = - np.sum(self.Y * np.log(self.aZ+self.epsilon))\n",
    "\n",
    "    def backward(self):\n",
    "    # Computes the gradient with respect to the predicted labels (dL/daZ).\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACTIVATION FUNCTIONS\n",
    "\n",
    "class _softmax: \n",
    "    def __init__(self, Z):\n",
    "        \"\"\"\n",
    "        Constructor for the softmax activation function.\n",
    "        \"\"\"\n",
    "        self.Z = Z #  Z (numpy.ndarray): Input matrix.\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation and stores the result in aZ.\n",
    "        \"\"\"\n",
    "        max_Z = np.max( self.Z, axis=1 ,keepdims=True )\n",
    "        self.aZ = (np.exp(self.Z - max_Z ))/np.sum( np.exp(self.Z - max_Z), axis=1 , keepdims=True)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))\n",
    "  \n",
    "\n",
    "class _sigmoid:\n",
    "    def __init__(self, Z):\n",
    "        \"\"\"\n",
    "        Constructor for the sigmoid activation function.\n",
    "        \"\"\"\n",
    "        self.Z = Z\n",
    "\n",
    "    def forward(self,):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation and stores the result in aZ.\n",
    "        \"\"\"\n",
    "        self.aZ = 1./(1 + np.exp(-self.Z))\n",
    "\n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        Backward pass of the sigmoid activation function.\n",
    "        Computes the Jacobian matrix of the sigmoid activation (daZ/dZ).\n",
    "        \"\"\"\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries)\n",
    "\n",
    "\n",
    "class _linear:\n",
    "    def __init__(self, Z):\n",
    "        \"\"\"\n",
    "        Constructor for the linear activation function.\n",
    "        \"\"\"\n",
    "        self.Z = Z # Z (numpy.ndarray): Input matrix.\n",
    "    \n",
    "    def forward(self, ):\n",
    "        \"\"\"\n",
    "        Forward pass of the linear activation function.\n",
    "        Directly sets aZ equal to Z.\n",
    "        \"\"\"\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        Backward pass of the linear activation function.\n",
    "        Computes the Jacobian matrix of the linear activation (daZ/dZ).\n",
    "        \"\"\"\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] )\n",
    "\n",
    "\n",
    "class _tanh:\n",
    "    def __init__(self, Z):\n",
    "        \"\"\"\n",
    "        Constructor for the hyperbolic tangent (tanh) activation function.\n",
    "        \"\"\"\n",
    "        self.Z = Z # Z (numpy.ndarray): Input matrix.\n",
    "\n",
    "    def forward(self,):\n",
    "        \"\"\"\n",
    "        Forward pass of the tanh activation function.\n",
    "        Computes the tanh activation and stores the result in aZ.\n",
    "        \"\"\"\n",
    "        self.aZ = np.tanh(self.Z)\n",
    "\n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        Backward pass of the tanh activation function.\n",
    "        Computes the Jacobian matrix of the tanh activation (daZ/dZ).\n",
    "        \"\"\"\n",
    "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2)\n",
    "\n",
    "\n",
    "class _relu:\n",
    "    def __init__(self, Z): \n",
    "        \"\"\"\n",
    "        Constructor for the rectified linear unit (ReLU) activation function.\n",
    "\n",
    "        Parameters:\n",
    "        Z (numpy.ndarray): Input matrix.\n",
    "        \"\"\"\n",
    "        self.Z = Z\n",
    "        self.Leak = 0.01\n",
    "    \n",
    "    def forward(self,):\n",
    "        \"\"\"\n",
    "        Forward pass of the ReLU activation function.\n",
    "        Computes the ReLU activation and stores the result in aZ.\n",
    "        \"\"\"\n",
    "        self.aZ = np.maximum(self.Z,0)\n",
    "    \n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        Backward pass of the ReLU activation function.\n",
    "        Computes the Jacobian matrix of the ReLU activation (daZ/dZ).\n",
    "        \"\"\"\n",
    "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='boston', normalize_X=False, normalize_y=False, one_hot_encode_y=False, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Load and preprocess different datasets based on the specified dataset_name.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): Name of the dataset to load ('boston', 'iris', 'mnist').\n",
    "    - normalize_X (bool): Flag to indicate whether to normalize the features (X).\n",
    "    - normalize_y (bool): Flag to indicate whether to normalize the target variable (y).\n",
    "    - one_hot_encode_y (bool): Flag to indicate whether to perform one-hot encoding on the target variable (y).\n",
    "    - test_size (float): Size of the test set when splitting the data.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (numpy.ndarray): Training features.\n",
    "    - y_train (numpy.ndarray): Training target variable.\n",
    "    - X_test (numpy.ndarray): Test features.\n",
    "    - y_test (numpy.ndarray): Test target variable.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'boston':\n",
    "        # Load Boston dataset from URL\n",
    "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "        data_boston = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "        result_boston = raw_df.values[1::2, 2]\n",
    "        data = {'data': data_boston, 'target': result_boston}  \n",
    "    elif dataset_name == 'iris':\n",
    "        # Load Iris dataset from sklearn\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'mnist':\n",
    "        # Load MNIST-like dataset from sklearn and binarize it\n",
    "        data = load_digits()\n",
    "        data['data'] = 1*(data['data'] >= 8)\n",
    "    # Extract features (X) and target variable (y)\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1, 1)\n",
    "    \n",
    "    # Normalize features if specified\n",
    "    if normalize_X == True:\n",
    "        normalizer = Normalizer()\n",
    "        X = normalizer.fit_transform(X)\n",
    "    \n",
    "    # Normalize target variable if specified\n",
    "    if normalize_y == True:\n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "    # One-hot encode target variable if specified\n",
    "    if one_hot_encode_y == True:\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer and Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, inSize, outSize, activation=\"linear\", seed=42):\n",
    "        \"\"\"\n",
    "        Constructor for the neural network layer.\n",
    "        \n",
    "        - inSize (int): Number of input features.\n",
    "        - outSize (int): Number of output features.\n",
    "        - activation (str): Activation function to use ('linear', 'relu', 'tanh', 'sigmoid', 'softmax').\n",
    "        - seed (int): Seed for random number generation.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.inSize = inSize\n",
    "        self.outSize = outSize\n",
    "\n",
    "        self.X = np.random.random((1, inSize))\n",
    "\n",
    "        self.W = np.random.random((inSize, outSize))\n",
    "        self.B = np.random.random((1, outSize))\n",
    "        \n",
    "        self.Z = np.random.random((1, outSize))\n",
    "\n",
    "        # Initialize sub-layers\n",
    "        self.mul_layer = _multiplication_layer(self.X, self.W)\n",
    "        self.bias_layer = _bias_addition_layer(self.B, self.B)\n",
    "\n",
    "        # Initialize activation layer based on specified activation function\n",
    "        if activation == 'linear':\n",
    "            self.activation_layer = _linear(self.Z)\n",
    "        elif activation == 'relu':\n",
    "            self.activation_layer = _relu(self.Z)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_layer = _tanh(self.Z)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_layer = _sigmoid(self.Z)\n",
    "        elif activation == 'softmax':\n",
    "            self.activation_layer = _softmax(self.Z)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "        \"\"\"\n",
    "        self.mul_layer.X = self.X\n",
    "        self.mul_layer.forward()\n",
    "\n",
    "        self.bias_layer.Z = self.mul_layer.Z\n",
    "        self.bias_layer.forward()\n",
    "\n",
    "        self.activation_layer.Z = self.bias_layer.Z\n",
    "        self.activation_layer.forward()\n",
    "\n",
    "        self.Z = self.activation_layer.aZ\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass of the layer.\n",
    "        \"\"\"\n",
    "        self.activation_layer.backward()\n",
    "        self.bias_layer.backward()\n",
    "        self.mul_layer.backward()\n",
    "\n",
    "\n",
    "class NeuralNetwork(Layer):\n",
    "\n",
    "    def __init__(self, layerList, loss=\"mean_squared\", lr=0.01, seed=42):\n",
    "        \"\"\"\n",
    "        Constructor for the neural network.\n",
    "\n",
    "        - layerList (list): List of Layer objects representing the network architecture.\n",
    "        - loss (str): Loss function to use ('mean_squared', 'cross_entropy').\n",
    "        - lr (float): Learning rate for optimization.\n",
    "        - seed (int): Seed for random number generation.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layerList = layerList\n",
    "        self.num_layers = len(layerList)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.inShape = self.layerList[0].X.shape\n",
    "        self.outShape = self.layerList[-1].Z.shape\n",
    "\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        # Initialize the loss layer based on the specified loss function\n",
    "        if loss == \"mean_squared\":\n",
    "            self.loss_layer = _mean_squared_error_loss(self.Y, self.Y)\n",
    "        if loss == \"cross_entropy\":\n",
    "            self.loss_layer = _cross_entropy_loss(self.Y, self.Y)\n",
    "\n",
    "    def forward(self):\n",
    "        self.layerList[0].X = self.X\n",
    "        self.loss_layer.Y = self.Y\n",
    "\n",
    "        self.layerList[0].forward()\n",
    "  \n",
    "        for i in range(1, self.num_layers):\n",
    "            self.layerList[i].X = self.layerList[i-1].Z\n",
    "            self.layerList[i].forward()\n",
    "\n",
    "        self.loss_layer.aZ = self.layerList[-1].Z\n",
    "        self.loss_layer.forward()\n",
    "\n",
    "    def backward(self):\n",
    "        self.loss_layer.Z = self.Y\n",
    "        self.loss_layer.backward()\n",
    "\n",
    "        self.grad_nn = self.loss_layer.dL_daZ\n",
    "\n",
    "        for i in range(self.num_layers-1, -1, -1):\n",
    "            self.layerList[i].backward()\n",
    "\n",
    "            dL_dZ = np.dot(self.layerList[i].activation_layer.daZ_dZ, self.grad_nn)\n",
    "            dL_dW = np.dot(self.layerList[i].mul_layer.dZ_dW, dL_dZ.T)\n",
    "            dL_dB = np.dot(self.layerList[i].bias_layer.dZ_dB, dL_dZ).T\n",
    "\n",
    "            self.layerList[i].W -= self.lr*dL_dW\n",
    "            self.layerList[i].B -= self.lr*dL_dB\n",
    "\n",
    "            self.grad_nn = np.dot(self.layerList[i].mul_layer.dZ_daZ_prev, dL_dZ)\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SGD for Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLayers(inShape, layerSize, activation):\n",
    "    layers = []\n",
    "    n_layers = len(layerSize)\n",
    "\n",
    "    for i in range(0, n_layers):\n",
    "        if i==0:\n",
    "            layer_i = Layer(inShape, layerSize[i], activation[i])\n",
    "        else:\n",
    "            layer_i = Layer(outShape, layerSize[i], activation[i])\n",
    "        layers.append(layer_i)\n",
    "        outShape = layerSize[i]\n",
    "\n",
    "    return inShape, outShape, layers\n",
    "\n",
    "def StocasticGradiantDescentArtificialNeuralNetwork(X_train, y_train, X_test, y_test, nn, inpShape=1, outShape=1, problem=\"regression\", epochs=500):\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inpShape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, outShape)\n",
    "\n",
    "        nn.X = X_sample\n",
    "        nn.Y = Y_sample\n",
    "\n",
    "        nn.forward()\n",
    "        nn.backward()\n",
    "\n",
    "    if problem == \"regression\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        train_error = nn.loss_layer.L\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        test_error = nn.loss_layer.L\n",
    "\n",
    "        print(\"Train Data MSE : %0.5f\" % train_error)\n",
    "        print(\"Test Data MSE  : %0.5f\" % test_error)\n",
    "\n",
    "    if problem == \"classification\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        \n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Training Data Accuracy : {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Testing Data : {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the Boston dataset\n",
    "X_train, y_train, X_test, y_test = load_data('boston', normalize_X=True, normalize_y=False, test_size=0.2)\n",
    "\n",
    "# 'boston': Specify the dataset to load as the Boston Housing dataset.\n",
    "# normalize_X=True: Normalize the features (X) using sklearn's Normalizer.\n",
    "# normalize_y=False: Do not normalize the target variable (y).\n",
    "# test_size=0.2: Set the test set size to 20% of the total dataset.\n",
    "\n",
    "# Resulting variables:\n",
    "# X_train: Training features after normalization.\n",
    "# y_train: Training target variable without normalization.\n",
    "# X_test: Test features after normalization.\n",
    "# y_test: Test target variable without normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLayers(inShape, layerSize, activation):\n",
    "    \"\"\"\n",
    "    Create a list of Layer objects to represent the layers of the neural network.\n",
    "    - inShape (int): Number of input features.\n",
    "    - layerSize (list): List of layer sizes for each layer in the network.\n",
    "    - activation (list): List of activation functions for each layer in the network.\n",
    "    - inShape (int): Number of input features.\n",
    "    - outShape (int): Number of output features.\n",
    "    - layers (list): List of Layer objects representing the layers of the network.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    n_layers = len(layerSize)\n",
    "\n",
    "    for i in range(0, n_layers):\n",
    "         # Create a new layer based on the specified parameters\n",
    "        if i==0:\n",
    "            layer_i = Layer(inShape, layerSize[i], activation[i])\n",
    "        else:\n",
    "            layer_i = Layer(outShape, layerSize[i], activation[i])\n",
    "        # Append the newly created layer to the list of layers\n",
    "        layers.append(layer_i)\n",
    "        \n",
    "        # Update outShape for the next iteration\n",
    "        outShape = layerSize[i]\n",
    "\n",
    "    return inShape, outShape, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data MSE : 54.54307\n",
      "Test Data MSE  : 37.15076\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of input features from the training features\n",
    "inShape = X_train.shape[1]\n",
    "\n",
    "# Specify the neural network architecture\n",
    "size = [1]  # Number of neurons in each layer\n",
    "layers_activations = ['linear']  # Activation function for each layer\n",
    "\n",
    "# Create the neural network layers using the specified architecture\n",
    "inShape, outShape, layers = makeLayers(inShape, size, layers_activations)\n",
    "\n",
    "# Initialize and train the neural network using Stochastic Gradient Descent\n",
    "# - NeuralNetwork(layers, \"mean_squared\", lr=0.1): Create a neural network instance with specified layers and loss function.\n",
    "# - inShape, outShape: Number of input and output features.\n",
    "# - epochs=10000: Number of training epochs.\n",
    "# - problem=\"regression\": Specify the type of problem as regression.\n",
    "StocasticGradiantDescentArtificialNeuralNetwork(X_train, y_train, X_test, y_test, NeuralNetwork(layers, \"mean_squared\", lr=0.1), inShape, outShape, epochs=10000,problem=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data MSE : 72.76918\n",
      "Test Data MSE  : 61.25141\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of input features from the training features\n",
    "inp_shape = X_train.shape[1]\n",
    "\n",
    "# Specify the neural network architecture\n",
    "size = [13, 1]  # Number of neurons in each layer\n",
    "layers_activations = ['sigmoid', 'linear']  # Activation function for each layer\n",
    "\n",
    "# Create the neural network layers using the specified architecture\n",
    "\n",
    "inp_shape, out_shape, layers = makeLayers(inp_shape, size, layers_activations)\n",
    "StocasticGradiantDescentArtificialNeuralNetwork(X_train, y_train, X_test, y_test, NeuralNetwork(layers, \"mean_squared\", lr=0.01),inp_shape, out_shape, epochs=1000, problem=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data MSE : 87.78990\n",
      "Test Data MSE  : 72.83930\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of input features from the training features\n",
    "inp_shape = X_train.shape[1]\n",
    "\n",
    "# Specify the neural network architecture\n",
    "size = [13,13,1]  # Number of neurons in each layer\n",
    "layers_activations = ['sigmoid', 'sigmoid', 'linear']  # Activation function for each layer\n",
    "\n",
    "# Create the neural network layers using the specified architecture\n",
    "inp_shape, out_shape, layers = makeLayers(inp_shape, size, layers_activations)\n",
    "\n",
    "StocasticGradiantDescentArtificialNeuralNetwork(X_train, y_train, X_test, y_test, NeuralNetwork(layers, 'mean_squared', lr=0.001), inp_shape, out_shape,epochs=1000,problem=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST-like dataset\n",
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy : 127/1257 = 10.103420843277645 %\n",
      "Testing Data : 51/540 = 9.444444444444445 %\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of input features from the training features\n",
    "inp_shape = X_train.shape[1]\n",
    "\n",
    "# Specify the neural network architecture\n",
    "layers_sizes = [89, 10]  # Number of neurons in each layer\n",
    "layers_activations = ['tanh', 'sigmoid']  # Activation function for each layer\n",
    "\n",
    "# Create the neural network layers using the specified architecture\n",
    "inp_shape, out_shape, layers = makeLayers(inp_shape, layers_sizes, layers_activations)\n",
    "\n",
    "# Specify the loss function for the neural network\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "# Initialize the neural network with specified layers, loss function, and learning rate\n",
    "nn = NeuralNetwork(layers, loss_nn, lr=0.1)\n",
    "\n",
    "StocasticGradiantDescentArtificialNeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,epochs=10000,problem=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy : 127/1257 = 10.103420843277645 %\n",
      "Testing Data : 51/540 = 9.444444444444445 %\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of input features from the training features\n",
    "inp_shape = X_train.shape[1]\n",
    "\n",
    "# Specify the neural network architecture\n",
    "layers_sizes = [89, 10]  # Number of neurons in each layer\n",
    "layers_activations = ['tanh', 'softmax']  # Activation function for each layer\n",
    "\n",
    "# Create the neural network layers using the specified architecture\n",
    "inp_shape, out_shape, layers = makeLayers(inp_shape, layers_sizes, layers_activations)\n",
    "\n",
    "# Specify the loss function for the neural network\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "# Initialize the neural network with specified layers, loss function, and learning rate\n",
    "nn = NeuralNetwork(layers, loss_nn, lr=0.01)\n",
    "\n",
    "# Train the neural network using Stochastic Gradient Descent\n",
    "StocasticGradiantDescentArtificialNeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,epochs=10000,problem=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Layer and Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    def __init__(self, inp_shape, activation='tanh', filter_shape=(1, 1), lr=0.01, Co=1, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize a Convolutional Layer.\n",
    "        - inp_shape (tuple): Shape of the input data (channels, height, width).\n",
    "        - activation (str): Activation function for the layer.\n",
    "        - filter_shape (tuple): Shape of the filters (channels, height, width).\n",
    "        - lr (float): Learning rate for gradient descent.\n",
    "        - Co (int): Number of output channels (number of filters).\n",
    "        - seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.inp = np.random.rand(*inp_shape)\n",
    "        self.inp_shape = inp_shape\n",
    "\n",
    "        self.Ci = self.inp.shape[0]\n",
    "        self.Co = Co\n",
    "        self.filters_shape = (self.Co, self.Ci, *filter_shape)\n",
    "        self.out_shape = ( self.Co, self.inp.shape[1] - filter_shape[0] + 1, self.inp.shape[2] - filter_shape[1] + 1)\n",
    "        self.flatten_shape = self.out_shape[0] * self.out_shape[1]*self.out_shape[2]\n",
    "        self.lr = lr\n",
    "\n",
    "        self.filters = np.random.rand(*self.filters_shape)\n",
    "        self.biases = np.random.rand(*self.out_shape)\n",
    "        self.out = np.random.rand(*self.out_shape)\n",
    "        self.flat = np.random.rand(1, self.flatten_shape)\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.activation_layer = _tanh(self.out)\n",
    "\n",
    "    def flatten(self):\n",
    "        \"\"\"\n",
    "        Flatten the output for further processing.\n",
    "        \"\"\"\n",
    "        self.flat = self.out.reshape(1, -1)\n",
    "        \n",
    "    def convolve(self, x, y):\n",
    "        \"\"\"\n",
    "        Perform convolution operation between two matrices x and y.\n",
    "        - x (numpy.ndarray): Input matrix.\n",
    "        - y (numpy.ndarray): Filter matrix.\n",
    "        \"\"\"\n",
    "        m = x.shape[0] - y.shape[0] + 1\n",
    "        n = x.shape[1] - y.shape[1] + 1\n",
    "        x_conv_y = np.zeros((m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                tmp = x[i:i+y.shape[0], j:j+y.shape[1]]\n",
    "                tmp = np.multiply(tmp, y)\n",
    "                x_conv_y[i, j] = np.sum(tmp)\n",
    "        return x_conv_y\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the convolutional layer.\n",
    "        \"\"\"\n",
    "        self.out = np.copy(self.biases)\n",
    "        for i in range(self.Co):\n",
    "            for j in range(self.Ci):\n",
    "                self.out[i] += self.convolve(self.inp[j], self.filters[i, j])\n",
    "\n",
    "        self.flatten()\n",
    "        self.activation_layer.Z = self.flat\n",
    "        self.activation_layer.forward()\n",
    "\n",
    "    def backward(self, grad_nn):\n",
    "        \"\"\"\n",
    "        Perform backward pass through the convolutional layer.\n",
    "        - grad_nn (numpy.ndarray): Gradient from the neural network.\n",
    "\n",
    "        Updates:\n",
    "        - self.filters: Update filter weights.\n",
    "        - self.biases: Update biases.\n",
    "        \"\"\"\n",
    "\n",
    "        self.activation_layer.backward()\n",
    "        loss_gradient = np.dot(self.activation_layer.daZ_dZ, grad_nn)\n",
    "        loss_gradient = np.reshape(loss_gradient, self.out_shape)\n",
    "        \n",
    "        self.filters_gradient = np.zeros(self.filters_shape)\n",
    "        self.input_gradient = np.zeros(self.inp_shape)\n",
    "        self.biases_gradient = loss_gradient\n",
    "        padded_loss_gradient = np.pad(loss_gradient, ((\n",
    "            0, 0), (self.filters_shape[2]-1, self.filters_shape[2]-1), (self.filters_shape[3]-1, self.filters_shape[3]-1)))\n",
    "\n",
    "        for i in range(self.Co):\n",
    "            for j in range(self.Ci):\n",
    "                self.filters_gradient[i, j] = self.convolve(\n",
    "                    self.inp[j], loss_gradient[i])\n",
    "                rot180_Kij = np.rot90(\n",
    "                    np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))\n",
    "                self.input_gradient[j] += self.convolve(\n",
    "                    padded_loss_gradient[i], rot180_Kij)\n",
    "\n",
    "        self.filters -= self.lr*self.filters_gradient\n",
    "        self.biases -= self.lr*self.biases_gradient\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetwork :\n",
    "    def __init__(self, convolutional_layer, nn, seed = 42): \n",
    "        \"\"\"\n",
    "        Initialize a Convolutional Neural Network.\n",
    "        - convolutional_layer (ConvolutionalLayer): Convolutional layer instance.\n",
    "        - nn (NeuralNetwork): Neural network instance.\n",
    "        - seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.nn = nn \n",
    "        self.convolutional_layer = convolutional_layer \n",
    "        self.X = None\n",
    "        self.Y = None \n",
    "    \n",
    "    def forward(self,): \n",
    "        \"\"\"\n",
    "        Perform forward pass through the Convolutional Neural Network.\n",
    "        \"\"\"\n",
    "        self.convolutional_layer.inp = self.X \n",
    "        self.convolutional_layer.forward()\n",
    " \n",
    "        self.nn.X = self.convolutional_layer.activation_layer.aZ\n",
    "        self.nn.Y = self.Y \n",
    "\n",
    "        self.nn.forward()  \n",
    "    \n",
    "    def backward(self,): \n",
    "        self.nn.backward() \n",
    "        \n",
    "        self.convolutional_layer.backward( self.nn.grad_nn ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StocasticGradiantDescentCNN(X_train, y_train, X_test, y_test, cnn, inShape, outShape, epochs=1000, problem=\"classification\"):\n",
    "    \"\"\"\n",
    "    Perform Stochastic Gradient Descent for training a Convolutional Neural Network.\n",
    "    - X_train (numpy.ndarray): Training features.\n",
    "    - y_train (numpy.ndarray): Training target variable.\n",
    "    - X_test (numpy.ndarray): Test features.\n",
    "    - y_test (numpy.ndarray): Test target variable.\n",
    "    - cnn (ConvolutionalNeuralNetwork): Convolutional Neural Network instance.\n",
    "    - inShape (tuple): Shape of the input data.\n",
    "    - outShape (tuple): Shape of the output data.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - problem (str): Type of problem, 'classification' or 'regression'.\n",
    "    \"\"\"\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        \n",
    "        # Extract a random sample from the training data\n",
    "        X_sample = X_train[randomIndx, :].reshape(inShape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(outShape)\n",
    "\n",
    "        # Set the input and target variables for the CNN\n",
    "        cnn.X = X_sample\n",
    "        cnn.Y = Y_sample\n",
    "\n",
    "        # Perform forward and backward passes through the CNN\n",
    "        cnn.forward()\n",
    "        cnn.backward()\n",
    "\n",
    "    # Evaluate training accuracy\n",
    "    X_train = X_train.reshape(-1, 8, 8)\n",
    "    y_true = np.argmax(y_train, axis=1)\n",
    "    acc = 0\n",
    "    for i in range(len(X_train)):\n",
    "        cnn.X = X_train[i][np.newaxis, :, :]\n",
    "        cnn.Y = y_train[i]\n",
    "        cnn.forward()\n",
    "        y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1)\n",
    "        if (y_pred_i == y_true[i]):\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"Training Data Accuracy :\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n",
    "    \n",
    "    # Evaluate testing accuracy\n",
    "    X_test = X_test.reshape(-1, 8, 8)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    acc = 0\n",
    "    for i in range(len(X_test)):\n",
    "        cnn.X = X_test[i][np.newaxis, :, :]\n",
    "        cnn.Y = y_test[i]\n",
    "        cnn.forward()\n",
    "        y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1)\n",
    "        if (y_pred_i == y_true[i]):\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"Testing Data Accuracy :\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy :1322/1437 = 91.9972164231037 %\n",
      "Testing Data Accuracy :331/360 = 91.94444444444444 %\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset with one-hot encoding for target variable\n",
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True)\n",
    "\n",
    "# Initialize the Convolutional Layer\n",
    "convolutional_layer = ConvolutionalLayer((1,8,8), filter_shape=(3,3), Co=16, activation='tanh', lr=0.01)\n",
    "\n",
    "# Extract the output shape of the convolutional layer for the Neural Network\n",
    "nn_inp_shape = convolutional_layer.flatten_shape\n",
    "\n",
    "# Specify the architecture of the Neural Network layers after the convolutional layer\n",
    "nn_inp_shape, nn_out_shape, layers = makeLayers(nn_inp_shape, [10], ['softmax'])\n",
    "\n",
    "# Create the Convolutional Neural Network with the initialized convolutional layer and neural network layers\n",
    "cnn = ConvolutionalNeuralNetwork(convolutional_layer, NeuralNetwork(layers, 'cross_entropy', lr=0.01))\n",
    "\n",
    "# Specify the output shape for the CNN\n",
    "out_shape = (1, layers_sizes[-1])\n",
    "\n",
    "# Perform Stochastic Gradient Descent training for the Convolutional Neural Network\n",
    "StocasticGradiantDescentCNN(X_train,y_train,X_test,y_test, cnn,(1,8,8), out_shape,epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
