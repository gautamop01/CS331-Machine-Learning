{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing , load_iris\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5416ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _matrix_multiplication:\n",
    "    def __init__(self, X, W): \n",
    "        self.X = X \n",
    "        self.W = W \n",
    "\n",
    "    def forward(self):\n",
    "        ### N = W @ X\n",
    "        self.N = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        ### dN/dW = X.T\n",
    "        self.dN_dW = (self.X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c881f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _bias_addition :\n",
    "    def __init__(self, WX, bias):\n",
    "        self.B = bias\n",
    "        self.WX = WX\n",
    "    \n",
    "    def forward(self):\n",
    "        ### WXpB = N + B\n",
    "        ### N = W @ X\n",
    "        self.WXpB = self.WX + self.B\n",
    "    \n",
    "    def backward(self):\n",
    "        ### dWXpB/dB = identity matrix\n",
    "        self.dWXpB_dB = np.identity(self.B.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _linear:\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.identity(self.Z.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f476834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _mean_squared_error : \n",
    "    def __init__(self ,Y ,Y_pred):\n",
    "        self.Y = Y \n",
    "        self.Y_pred = Y_pred\n",
    "    \n",
    "    def forward(self):\n",
    "        ### mean squared loss is (Y_predicted - Y)**2\n",
    "        self.L = np.mean((self.Y_pred - self.Y)**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        ### dL/dY_pred = (1/M)*(2*(Y_pred - Y))\n",
    "        ### M = no of training examples\n",
    "        self.dL_daZ = (2*(self.Y_pred - self.Y).T)/len(self.Y)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed79a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _softmax:\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "    \n",
    "    ### standalone function (do not require any information from class) but semantically somehow related to the class so staticmethod is used\n",
    "    ### without @staticmethod, error was coming \n",
    "    @staticmethod\n",
    "    def _softmax(Z):\n",
    "        max_Z = np.max(Z, axis=1, keepdims=True )\n",
    "        return (np.exp(Z - max_Z))/np.sum(np.exp(Z - max_Z), axis=1, keepdims=True)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.aZ = self._softmax(self.Z)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.daZ_dZ = np.diag(self.aZ.reshape(-1))-(self.aZ.T)@((self.aZ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60de0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _sigmoid:\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid(self.Z)\n",
    "\n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries) \n",
    "    \n",
    "    def sigmoid(Z):\n",
    "        return  1./(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd7235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _cross_entropy_loss :\n",
    "    def __init__(self, Y, Y_pred): \n",
    "        self.eps = 1e-40\n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "    \n",
    "    def forward(self):\n",
    "        self.L = -np.sum(self.Y * np.log(self.aZ+self.eps))\n",
    "        \n",
    "    def backward(self):\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.eps)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(dataset_name='california', normalize_X=False, one_hot = False, test_size=0.3):\n",
    "    \n",
    "    ### loading dataset as per requirement\n",
    "    if dataset_name == 'california' : \n",
    "        data = fetch_california_housing()\n",
    "    elif dataset_name == 'iris' : \n",
    "        data = load_iris()\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1,1)\n",
    "    \n",
    "    ### Normalizer used for normalizing the dataset\n",
    "\n",
    "    if normalize_X == True : \n",
    "        normalizer = Normalizer()\n",
    "        X  = normalizer.fit_transform(X)\n",
    "    \n",
    "    ### Conversion of y to one hot encoding as per demand\n",
    "    if one_hot == True : \n",
    "        y = np.eye(3)[y.reshape(-1)]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(X_sample, Y_sample, W, B, act='linear', loss='mean_squared'):\n",
    "    \n",
    "    ### matrix multiplication\n",
    "    updateLayer = _matrix_multiplication(X_sample, W)\n",
    "    updateLayer.forward()\n",
    "\n",
    "    ### adding bias\n",
    "    addBias = _bias_addition(updateLayer.N, B)\n",
    "    addBias.forward()\n",
    "\n",
    "    ### activation layer as per requirement\n",
    "    if act == 'softmax': \n",
    "        actLayer = _softmax(addBias.WXpB)\n",
    "    elif act == 'linear' : \n",
    "        actLayer = _linear(addBias.WXpB)\n",
    "\n",
    "    ### activation\n",
    "    actLayer.forward()\n",
    "    \n",
    "    ### loss function as per requirement\n",
    "    if loss=='cross_entropy':\n",
    "        lossLayer = _cross_entropy_loss(Y_sample, actLayer.aZ )\n",
    "    elif loss == 'mean_squared':\n",
    "        lossLayer = _mean_squared_error(Y_sample, actLayer.aZ )\n",
    "    \n",
    "    ### finding loss\n",
    "    lossLayer.forward()\n",
    "    \n",
    "    return updateLayer, addBias, actLayer, lossLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backward(updateLayer, addBias, actLayer, lossLayer): \n",
    "\n",
    "    ### returning all the losses\n",
    "    ### not necessarily required\n",
    "    lossLayer.backward()\n",
    "    actLayer.backward()\n",
    "    addBias.backward()\n",
    "    updateLayer.backward()\n",
    "\n",
    "    return lossLayer, actLayer, addBias, updateLayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_q3 = []\n",
    "test_acc_q3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradiantDescent(X_train, y_train, X_test, y_test, inShape = 1, seed = 42, n_iters = 100, outShape= 1, learning_rate = 0.001, activation = 'linear', loss = 'mean_squared', problem='regression'):\n",
    "\n",
    "    ### random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ### initializing weights and biases randomly\n",
    "    W = np.random.random((inShape, outShape))\n",
    "    B  = np.random.random((1, outShape))\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        for j in range(len(X_train)): \n",
    "            X_sample = X_train[j, :].reshape(1, inShape)\n",
    "            Y_sample = y_train[j, :].reshape(1, outShape)\n",
    "\n",
    "            ### Forward Pass\n",
    "            updateLayer, addBias, actLayer, lossLayer = _forward(X_sample, Y_sample, W, B, activation,loss)\n",
    "\n",
    "            ### Backward Pass \n",
    "            lossLayer, actLayer, addBias, updateLayer = _backward(updateLayer, addBias, actLayer, lossLayer)\n",
    "\n",
    "            dL_daZ = lossLayer.dL_daZ \n",
    "            ### print(dL_daZ)\n",
    "            dL_dZ = np.dot( actLayer.daZ_dZ, dL_daZ ) \n",
    "            ### print(dL_dZ)\n",
    "            dL_dW = np.dot( updateLayer.dN_dW , dL_dZ.T)\n",
    "            ### print(dL_dW)\n",
    "            dL_dB = np.dot( addBias.dWXpB_dB, dL_dZ).T\n",
    "            ### print(dL_dB)\n",
    "\n",
    "            ### Update the weights and bias\n",
    "            W -=  learning_rate*dL_dW \n",
    "            B -=  learning_rate*dL_dB\n",
    "\n",
    "    if problem =='classification': \n",
    "        ### TRAINING\n",
    "        ### truth value\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        _, _, _, lossLayer = _forward( X_train, y_train , W, B, activation, loss)\n",
    "\n",
    "        ### predicted value\n",
    "        y_pred = np.argmax(lossLayer.aZ, axis=1)\n",
    "\n",
    "        ### finding accuracy\n",
    "        acc = (y_pred == y_true)\n",
    "        print(f\"Train Accuracy: {sum(acc)*100/len(acc)} %\")\n",
    "\n",
    "        train_acc_q3.append(sum(acc)*100/len(acc))\n",
    "\n",
    "        ### TESTING\n",
    "        ### truth value\n",
    "        y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "        ### predicted value\n",
    "        _, _, _, lossLayer = _forward( X_test, y_test , W, B, activation, loss)\n",
    "        y_pred = np.argmax( lossLayer.aZ, axis=1)\n",
    "\n",
    "        ### finding accuracy\n",
    "        acc = (y_pred == y_true)\n",
    "        print(f\"Test Accuracy: {sum(acc)*100/len(acc)} %\")\n",
    "        test_acc_q3.append(sum(acc)*100/len(acc))\n",
    "\n",
    "        return train_acc_q3, test_acc_q3\n",
    "            \n",
    "    if problem =='regression':\n",
    "        ### TRAINING\n",
    "        _ , _, _, lossLayer = _forward( X_train, y_train , W, B, activation, loss)\n",
    "        print(f\"Train Error : {lossLayer.L}\")\n",
    "                    \n",
    "        ### TESTING\n",
    "        _ , _, _, lossLayer = _forward( X_test, y_test , W, B, activation, loss)\n",
    "        print(f\"Test Error : {lossLayer.L}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = load('california', normalize_X=True, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error : 1.2934855427739083\n",
      "Test Error : 1.3186464066060344\n"
     ]
    }
   ],
   "source": [
    "GradiantDescent(X_train, y_train, X_test, y_test, inShape=X_train.shape[1], outShape=y_train.shape[1], problem='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load('iris', normalize_X=True, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n"
     ]
    }
   ],
   "source": [
    "trq3, tesq3 = GradiantDescent(X_train, y_train, X_test, y_test, inShape=X_train.shape[1], outShape=y_train.shape[1], n_iters=50, learning_rate=0.03, activation='softmax', problem='classification', loss='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = np.linspace(0, 1, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99, 1.  ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "Train Accuracy: 42.857142857142854 %\n",
      "Test Accuracy: 33.333333333333336 %\n",
      "0.01 1\n",
      "Train Accuracy: 82.85714285714286 %\n",
      "Test Accuracy: 77.77777777777777 %\n",
      "0.02 2\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 86.66666666666667 %\n",
      "0.03 3\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.04 4\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.05 5\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.06 6\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.07 7\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.08 8\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.09 9\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.1 10\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.11 11\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.12 12\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.13 13\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.14 14\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.15 15\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.16 16\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.17 17\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.18 18\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.19 19\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.2 20\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.21 21\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.22 22\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.23 23\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.24 24\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.25 25\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.26 26\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.27 27\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.28 28\n",
      "Train Accuracy: 98.0952380952381 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.29 29\n",
      "Train Accuracy: 98.0952380952381 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.3 30\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.31 31\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.32 32\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.33 33\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.34 34\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.35000000000000003 35\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.36 36\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.37 37\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.38 38\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.39 39\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.4 40\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.41000000000000003 41\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.42 42\n",
      "Train Accuracy: 97.14285714285714 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.43 43\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.44 44\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.45 45\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.46 46\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.47000000000000003 47\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.48 48\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 95.55555555555556 %\n",
      "0.49 49\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.5 50\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.51 51\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.52 52\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.53 53\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.54 54\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.55 55\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.56 56\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.5700000000000001 57\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.58 58\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.59 59\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.6 60\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.61 61\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.62 62\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.63 63\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.64 64\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.65 65\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.66 66\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.67 67\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.68 68\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.6900000000000001 69\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.7000000000000001 70\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.71 71\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.72 72\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.73 73\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.74 74\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.75 75\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.76 76\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.77 77\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.78 78\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.79 79\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.8 80\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.81 81\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.8200000000000001 82\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.8300000000000001 83\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.84 84\n",
      "Train Accuracy: 94.28571428571429 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.85 85\n",
      "Train Accuracy: 95.23809523809524 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.86 86\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.87 87\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.88 88\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.89 89\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.9 90\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.91 91\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.92 92\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.93 93\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.9400000000000001 94\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.9500000000000001 95\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.96 96\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.97 97\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.98 98\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "0.99 99\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n",
      "1.0 100\n",
      "Train Accuracy: 96.19047619047619 %\n",
      "Test Accuracy: 93.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(len(ls)):\n",
    "    print(ls[i], i)\n",
    "    trainq3, testq3 = GradiantDescent(X_train, y_train, X_test, y_test, inShape=X_train.shape[1], outShape=y_train.shape[1], n_iters=50, learning_rate=ls[i], activation='softmax', problem='classification', loss='cross_entropy')\n",
    "    train_acc.append(trainq3)\n",
    "    test_acc.append(testq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.19047619047619"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc[0].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(ls, train_acc[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(ls, train_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.33333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc[0].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(ls, test_acc[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(ls, test_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conclusion: Best learning rate is around 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f43d1047cbef59950c2f25d14b04d8a01f164d85880f1357fcf1ba55dd5a278d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
